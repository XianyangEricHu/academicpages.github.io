{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c1543e",
   "metadata": {},
   "source": [
    "# Problem 1: MLE for GARCH/TGARCH Parameter Estimation\n",
    "\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d1f5a",
   "metadata": {},
   "source": [
    "Given $$\\sigma_t^2 = \\omega + \\alpha \\cdot \\epsilon_{t-1}^2 + \\beta \\cdot \\sigma_{t-1}^2$$\n",
    "\n",
    "We want to maximize\n",
    "\n",
    "$$\\log L = -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log(2\\pi) + \\log(\\sigma_t^2) + \\frac{\\epsilon_t^2}{\\sigma_t^2} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d456278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Estimated Parameters: [0.01921922 0.10808238 0.87787505]\n",
      "Maximum Log-Likelihood: -10551.148052375826\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import yfinance as yf\n",
    "\n",
    "# Step 1: Fetch SPY data\n",
    "spy_data = yf.download('SPY', start='1993-02-01', end='2023-10-20')\n",
    "\n",
    "# Step 2: Calculate and rescale log returns\n",
    "spy_data['log_return'] = 100 * np.log(spy_data['Adj Close'] / spy_data['Adj Close'].shift(1))  # Rescaling by 100\n",
    "returns = spy_data['log_return'].dropna()\n",
    "\n",
    "# Step 3: Define the GARCH(1,1) likelihood function\n",
    "def garch_likelihood(params, data):\n",
    "    alpha0, alpha1, beta1 = params\n",
    "    if alpha1 + beta1 >= 1:  # Enforcing stationarity\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    sigma2 = np.full(n, np.var(data))  # More stable initialization\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    for t in range(1, n):\n",
    "        sigma2[t] = max(alpha0 + alpha1 * data[t-1]**2 + beta1 * sigma2[t-1], 1e-8)\n",
    "        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(sigma2[t]) + data[t]**2 / sigma2[t])\n",
    "\n",
    "    return -log_likelihood\n",
    "\n",
    "# Step 4: Optimize the GARCH(1,1) parameters\n",
    "# Adjust initial_params to reflect the rescaling\n",
    "initial_params = [0.1, 0.05, 0.9]  # Adjusted initial guess for alpha0\n",
    "result = minimize(garch_likelihood, initial_params, args=(returns.values,), method='SLSQP', bounds=[(0, None), (0, 1), (0, 1)], options={'maxiter': 10000, 'ftol': 1e-10})\n",
    "\n",
    "# Step 5: Output the results\n",
    "if result.success:\n",
    "    fitted_params = result.x\n",
    "    max_likelihood = -result.fun\n",
    "    print(\"Estimated Parameters:\", fitted_params)\n",
    "    print(\"Maximum Log-Likelihood:\", max_likelihood)\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45362fff",
   "metadata": {},
   "source": [
    "The SBC/BIC is computed as:\n",
    "\n",
    "$$ \\text{BIC} = k \\ln(n) - 2 \\ln(\\hat{L}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb298c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC: 21129.156637040673\n"
     ]
    }
   ],
   "source": [
    "k = len(fitted_params)\n",
    "n = len(returns)\n",
    "bic = k * np.log(n) - 2 * max_likelihood\n",
    "print(\"BIC:\", bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1285d4a",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619c5149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Eric/opt/anaconda3/envs/dsm/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:10348: RuntimeWarning: overflow encountered in power\n",
      "  return np.log(0.5*beta) - sc.gammaln(1.0/beta) - abs(x)**beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Parameters: [0.01570085 0.11505633 0.88512925 1.33235826]\n",
      "Maximum Log-Likelihood: -10374.577239663622\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import gennorm\n",
    "\n",
    "def garch_ged_likelihood(params, data):\n",
    "    alpha0, alpha1, beta1, nu = params\n",
    "    n = len(data)\n",
    "    sigma2 = np.full(n, np.var(data))  # Stable initialization\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    for t in range(1, n):\n",
    "        sigma2[t] = max(alpha0 + alpha1 * data[t-1]**2 + beta1 * sigma2[t-1], 1e-8)\n",
    "        pdf_val = gennorm.pdf(data[t], nu, scale=np.sqrt(sigma2[t]))\n",
    "        if pdf_val <= 0:\n",
    "            return np.inf  # Penalize invalid PDF values\n",
    "        log_likelihood += np.log(pdf_val)\n",
    "\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "alpha0_guess = 0.1 \n",
    "alpha1_guess = 0.05 \n",
    "beta1_guess = 0.9  \n",
    "nu_guess = 2.0      \n",
    "\n",
    "initial_params = [alpha0_guess, alpha1_guess, beta1_guess, nu_guess]\n",
    "\n",
    "result = minimize(garch_ged_likelihood, initial_params, args=(returns.values,), method='SLSQP', bounds=[(0, None), (0, 1), (0, 1), (1, None)])\n",
    "\n",
    "if result.success:\n",
    "    fitted_params = result.x\n",
    "    max_likelihood = -result.fun\n",
    "    print(\"Estimated Parameters:\", fitted_params)\n",
    "    print(\"Maximum Log-Likelihood:\", max_likelihood)\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4628dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC: 20784.968522379273\n"
     ]
    }
   ],
   "source": [
    "k = len(fitted_params)\n",
    "n = len(returns)\n",
    "bic = k * np.log(n) - 2 * max_likelihood\n",
    "print(\"BIC:\", bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070217cb",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "Since the BIC of the GARCH-GED model is lower, it suggests that the improved fit (higher likelihood) compensates for the added complexity of the extra parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a86d4e",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a322b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGARCH(1,1) Estimated Parameters: [0.02241267 0.00554826 0.17445458 0.89035083]\n",
      "TGARCH(1,1) Maximum Log-Likelihood: -10404.512639651963\n",
      "BIC: 20844.839322355954\n"
     ]
    }
   ],
   "source": [
    "def tgarch_likelihood(params, data):\n",
    "    alpha0, alpha1, gamma, beta1 = params\n",
    "    # We'll handle the stationarity condition in the bounds\n",
    "    n = len(data)\n",
    "    sigma2 = np.full(n, np.var(data))\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    for t in range(1, n):\n",
    "        indicator = 1 if data[t-1] < 0 else 0\n",
    "        sigma2[t] = max(alpha0 + (alpha1 + gamma * indicator) * data[t-1]**2 + beta1 * sigma2[t-1], 1e-8)\n",
    "        log_likelihood += -0.5 * (np.log(2 * np.pi) + np.log(sigma2[t]) + data[t]**2 / sigma2[t])\n",
    "\n",
    "    return -log_likelihood\n",
    "\n",
    "# Adjusted initial parameters and bounds\n",
    "initial_params_tgarch = [0.1, 0.05, 0.05, 0.8]  # Adjust these as needed\n",
    "bounds_tgarch = [(1e-6, None), (0, 0.5), (0, 0.5), (0, 0.99)]  # Looser bounds, ensuring stationarity\n",
    "\n",
    "result_tgarch = minimize(tgarch_likelihood, initial_params_tgarch, args=(returns.values,), method='SLSQP', bounds=bounds_tgarch, options={'maxiter': 100000, 'ftol': 1e-6})\n",
    "\n",
    "if result_tgarch.success:\n",
    "    fitted_params_tgarch = result_tgarch.x\n",
    "    max_likelihood_tgarch = -result_tgarch.fun\n",
    "    print(\"TGARCH(1,1) Estimated Parameters:\", fitted_params_tgarch)\n",
    "    print(\"TGARCH(1,1) Maximum Log-Likelihood:\", max_likelihood_tgarch)\n",
    "else:\n",
    "    print(\"TGARCH(1,1) Optimization failed:\", result_tgarch.message)\n",
    "\n",
    "k_tgarch = 4  \n",
    "n = len(returns)  \n",
    "bic_tgarch = -2 * max_likelihood_tgarch + k_tgarch * np.log(n)\n",
    "print(\"BIC:\", bic_tgarch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451e99a",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "Since the BIC of the TGARCH(1,1) model is lower, it suggests that the improved fit (higher likelihood) compensates for the added complexity of the extra parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5b04e",
   "metadata": {},
   "source": [
    "# Probelm 2: Straddle Pricing with GARCH/TGARCH\n",
    "\n",
    "One of the big trends in options this year is the dramatic rise of trading in zero-daysto-expiry (0DTE) options . These are options that expire within 24 hours. Options\n",
    "that expire every day of the week are now available for popular securities like SPY and\n",
    "QQQ. According to the CBOE, 44% of the S&P500 option volume is for options with\n",
    "less than 24 hours to expiration, for a notional amount of over $500bn traded per day.\n",
    "The figure below lists prices and implied volatilities for puts and calls on SPY options\n",
    "as of the close on Friday, October 20, 2023. The middle column are the strikes, and\n",
    "the data on the left side are for the call options, and the data on the right side are for\n",
    "the put options. The first five rows are for options that expire the following Monday,\n",
    "one trading day later.\n",
    "\n",
    "Use the GARCH model in part (a) and the TGARCH model in part (c) above to\n",
    "price the 421 straddle (a straddle is a combination of a put and call at the same strike\n",
    "and expiration). You should price the straddle using Monte Carlo simulations over a\n",
    "one-day horizon. Explain how you get the volatility forecast from Friday to Monday.\n",
    "Notice that the market price of this straddle is 2.05 + 2.10 = 4.15. Do the models want\n",
    "to buy or sell the straddle? The TGARCH model should place a higher value on these\n",
    "options. Give a one sentence explanation why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45bf29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.584295767852295, 6.747386510593331)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Parameters for GARCH(1,1) and TGARCH\n",
    "params_garch = [0.01921938, 0.10808369, 0.87787375]\n",
    "params_tgarch = [0.02241294, 0.00554883, 0.17445635, 0.89034935]\n",
    "\n",
    "# Fetch SPY data\n",
    "spy_data = yf.download('SPY', start='1993-02-01', end='2023-10-20')\n",
    "spy_data['log_return'] = 100 * np.log(spy_data['Adj Close'] / spy_data['Adj Close'].shift(1))  # Rescaling by 100\n",
    "returns = spy_data['log_return'].dropna()\n",
    "\n",
    "# Last known price and return\n",
    "last_price = spy_data['Adj Close'].iloc[-1]\n",
    "last_return = returns.iloc[-1]\n",
    "\n",
    "# Number of simulations\n",
    "num_simulations = 10000\n",
    "\n",
    "# Adjusted functions to simulate log returns with checks for non-negative variance\n",
    "\n",
    "# Function to simulate log returns using GARCH(1,1) model with non-negative variance check\n",
    "def simulate_garch_fixed(params, last_return, num_simulations, initial_variance):\n",
    "    alpha0, alpha1, beta1 = params\n",
    "    sim_returns = np.zeros(num_simulations)\n",
    "    variance = initial_variance\n",
    "    \n",
    "    for i in range(num_simulations):\n",
    "        random_shock = np.random.normal(0, np.sqrt(max(variance, 1e-8)))\n",
    "        variance = alpha0 + alpha1 * last_return**2 + beta1 * variance\n",
    "        sim_returns[i] = random_shock * np.sqrt(max(variance, 1e-8))\n",
    "        \n",
    "    return sim_returns\n",
    "\n",
    "# Function to simulate log returns using TGARCH model with non-negative variance check\n",
    "def simulate_tgarch_fixed(params, last_return, num_simulations, initial_variance):\n",
    "    alpha0, alpha1, gamma, beta1 = params\n",
    "    sim_returns = np.zeros(num_simulations)\n",
    "    variance = initial_variance\n",
    "    \n",
    "    for i in range(num_simulations):\n",
    "        indicator = 1 if last_return < 0 else 0\n",
    "        random_shock = np.random.normal(0, np.sqrt(max(variance, 1e-8)))\n",
    "        variance = alpha0 + (alpha1 + gamma * indicator) * last_return**2 + beta1 * variance\n",
    "        sim_returns[i] = random_shock * np.sqrt(max(variance, 1e-8))\n",
    "        \n",
    "    return sim_returns\n",
    "\n",
    "# Assume an initial variance based on the long-term average variance of the returns\n",
    "initial_variance = np.var(returns)\n",
    "\n",
    "# Straddle strike price\n",
    "strike_price = 421\n",
    "\n",
    "# Simulate future log returns with adjusted functions\n",
    "sim_returns_garch_fixed = simulate_garch_fixed(params_garch, last_return, num_simulations, initial_variance)\n",
    "sim_returns_tgarch_fixed = simulate_tgarch_fixed(params_tgarch, last_return, num_simulations, initial_variance)\n",
    "\n",
    "# Calculate future prices with adjusted simulations\n",
    "future_prices_garch_fixed = last_price * np.exp(sim_returns_garch_fixed / 100)  # Rescaling back\n",
    "future_prices_tgarch_fixed = last_price * np.exp(sim_returns_tgarch_fixed / 100)  # Rescaling back\n",
    "\n",
    "# Calculate payouts and average price of the straddle for both models\n",
    "straddle_price_garch_fixed = np.mean(np.abs(future_prices_garch_fixed - strike_price))\n",
    "straddle_price_tgarch_fixed = np.mean(np.abs(future_prices_tgarch_fixed - strike_price))\n",
    "\n",
    "straddle_price_garch_fixed, straddle_price_tgarch_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccb3d9",
   "metadata": {},
   "source": [
    "Comparing these to the market price of the straddle, which is $\\$4.15$, we can infer the following:\n",
    "\n",
    "**GARCH Model**: The model's price of $\\$5.62$ is higher than the market price. This suggests that, according to the GARCH model, the straddle is undervalued in the market, and it would be a buy signal.\n",
    "\n",
    "**TGARCH Model**: With an even higher model price of $\\$6.69$, the TGARCH model also indicates that the straddle is undervalued in the market, suggesting a buy.\n",
    "The TGARCH model assigns a higher value to the straddle compared to the GARCH model. This is likely because the TGARCH model accounts for leverage effects, where negative returns increase future volatility more than positive returns. In volatile markets, this often results in higher option valuations under the TGARCH model, as options become more valuable with increased volatility expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dae73f",
   "metadata": {},
   "source": [
    "# Problem 3: Verify  OLS as a special case of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4125f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM Estimated alpha: 1.2221641996552628\n",
      "GMM Estimated beta: 1.993691454046184\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Set the true values for alpha and beta for the data simulation\n",
    "alpha_true = 1\n",
    "beta_true = 2\n",
    "N = 100  # Number of data points\n",
    "sigma = 1  # Standard deviation of the error terms\n",
    "\n",
    "# Simulate the independent variable x and the error terms\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(N) * 10  # Let's take x to be uniformly distributed between 0 and 10\n",
    "errors = np.random.normal(0, sigma, N)\n",
    "\n",
    "# Simulate the dependent variable y\n",
    "y = alpha_true + beta_true * x + errors\n",
    "\n",
    "# Define the moment conditions for GMM\n",
    "def moment_conditions(params, x, y):\n",
    "    alpha, beta = params\n",
    "    residuals = y - (alpha + beta * x)\n",
    "    m1 = residuals  # Moment condition based on the error term\n",
    "    m2 = x * residuals  # Moment condition based on the error term times x\n",
    "    return np.array([np.mean(m1), np.mean(m2)])\n",
    "\n",
    "# Objective function for GMM is the squared sum of moment conditions\n",
    "def gmm_objective(params, x, y):\n",
    "    moments = moment_conditions(params, x, y)\n",
    "    return np.sum(moments**2)\n",
    "\n",
    "# Initial guesses for alpha and beta\n",
    "initial_guess = [0, 0]\n",
    "\n",
    "# Minimize the GMM objective to find the estimated alpha and beta\n",
    "gmm_result = minimize(gmm_objective, initial_guess, args=(x, y))\n",
    "\n",
    "# The GMM estimated parameters are in gmm_result.x\n",
    "gmm_estimated_alpha, gmm_estimated_beta = gmm_result.x\n",
    "\n",
    "# Print the estimated parameters\n",
    "print(f'GMM Estimated alpha: {gmm_estimated_alpha}')\n",
    "print(f'GMM Estimated beta: {gmm_estimated_beta}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383b1b6",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf3c9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.22215108, 1.9936935 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant term to x to include an intercept in the model\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "# Create the OLS model and fit it to the data\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Retrieve and print the estimated parameters from the OLS model\n",
    "ols_estimated_params = results.params\n",
    "ols_estimated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981af05f",
   "metadata": {},
   "source": [
    "# Problem 4: Estimate CKLS Short-Term Parameters\n",
    "\n",
    "In the paper “An Empirical Comparison of Alternative Models of the Short-Term Interest Rate” by Chan, Karolyi, Longstaff, and Sanders (CKLS), they estimate parameters for eight popular models of short-term interest rates using GMM. In this exercise, you will replicate their study on one of those models.\n",
    "\n",
    "(a) Estimate the four parameters of the unrestricted model.\n",
    "\n",
    "(b) Estimate the two parameters in the Geometric Brownian Motion (GBM) model.\n",
    "\n",
    "Just like with the other excercises, I would like you to estimate the parameters from\n",
    "scratch, rather than using a package. I have attached one-month interest rate data\n",
    "from WRDS (the start date of 1964-06-30 and the end date of 1989-12-29 was chosen\n",
    "to match CKLS). I am also attaching the original CKLS paper. I was able to match\n",
    "the parameters of Table III of their paper relatively closely, but not exactly. In order\n",
    "to match the parameters, the data needs to be scaled correctly. First of all, you should\n",
    "divide the interest rates by 100, since 5% is represented as 5.0 and not 0.05. You should\n",
    "also include ∆t = 1/12 in the moment conditions. The first moment condition, for example, should be:\n",
    "\n",
    "$$E[r_{t+1} − r_t − (α + βr_t)∆t] = 0$$\n",
    "\n",
    "and the second moment condition should be:\n",
    "\n",
    "$$E[ϵ_{t+1}^2 − σ^2 r_t^{2γ}∆t] = 0$$\n",
    "\n",
    "Also, to make things easier, you can choose to do two-step GMM rather than iterated\n",
    "GMM, and you can compute the weighting matrix assuming the data are uncorrelated\n",
    "so you don’t have to compute the Newey-West adjustment if you would like. You may\n",
    "have to vary the initial parameter guesses because sometimes the optimizer gets stuck\n",
    "in a local maximum that is not the global maximum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1476bb8",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcea05d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters (two-step GMM): [ 0.04137383 -0.60433151  1.57585062  1.48397778]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/Eric/Downloads/Tbill_1mo.csv' \n",
    "interest_rate_data = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare the data by scaling the interest rates\n",
    "interest_rate_data['scaled_rate'] = interest_rate_data['ave_1'] / 100\n",
    "\n",
    "# Calculate the time step for monthly data\n",
    "delta_t = 1 / 12\n",
    "\n",
    "# Get the interest rates from the data\n",
    "interest_rates = interest_rate_data['scaled_rate'].values\n",
    "\n",
    "# Define the moment conditions\n",
    "def moment_conditions(params, rates, delta_t):\n",
    "    alpha, beta, sigma_sq, gamma = params\n",
    "    residuals_first = rates[1:] - rates[:-1] - (alpha + beta * rates[:-1]) * delta_t\n",
    "    residuals_second = residuals_first * rates[:-1]\n",
    "    residuals_third = (residuals_first**2 - sigma_sq * (rates[:-1]**(2 * gamma)) * delta_t)\n",
    "    return np.column_stack((residuals_first, residuals_second, residuals_third))\n",
    "\n",
    "# Objective function for GMM\n",
    "def gmm_objective(params, rates, delta_t, weight_matrix):\n",
    "    model_residuals = moment_conditions(params, rates, delta_t)\n",
    "    g_bar = np.mean(model_residuals, axis=0)\n",
    "    return g_bar @ weight_matrix @ g_bar\n",
    "\n",
    "# Initial parameter guesses\n",
    "initial_params = np.array([0, 0, 1.5, 2])\n",
    "\n",
    "# Identity matrix for the first step\n",
    "weight_identity = np.eye(3)\n",
    "\n",
    "# First step optimization using the identity matrix for weighting\n",
    "result_first_step = minimize(\n",
    "    fun=gmm_objective,\n",
    "    x0=initial_params,\n",
    "    args=(interest_rates, delta_t, weight_identity),\n",
    "    method='L-BFGS-B',\n",
    "    bounds=[(None, None), (None, None), (0, None), (0, None)]\n",
    ")\n",
    "\n",
    "if not result_first_step.success:\n",
    "    raise Exception(\"First step optimization failed.\")\n",
    "\n",
    "# Calculate the optimal weighting matrix using the residuals from the first step\n",
    "model_residuals_first_step = moment_conditions(result_first_step.x, interest_rates, delta_t)\n",
    "S_matrix = np.mean([np.outer(resid, resid) for resid in model_residuals_first_step], axis=0)\n",
    "optimal_weight = np.linalg.inv(S_matrix)\n",
    "\n",
    "# Second step optimization using the optimal weighting matrix\n",
    "result_second_step = minimize(\n",
    "    fun=gmm_objective,\n",
    "    x0=result_first_step.x,\n",
    "    args=(interest_rates, delta_t, optimal_weight),\n",
    "    method='L-BFGS-B',\n",
    "    bounds=[(None, None), (None, None), (0, None), (0, None)]\n",
    ")\n",
    "\n",
    "if result_second_step.success:\n",
    "    estimated_params = result_second_step.x\n",
    "    print(f\"Estimated parameters (two-step GMM): {estimated_params}\")\n",
    "else:\n",
    "    raise Exception(\"Second step optimization failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961785f9",
   "metadata": {},
   "source": [
    "The parameters for the unrestricted model have been estimated using the two-step GMM approach. The estimated parameters are as follows:\n",
    "1. $ \\alpha $: $0.04137383$\n",
    "2. $ \\beta $: $-0.60433151$\n",
    "3. $ \\sigma^2 $: $1.57585062$\n",
    "4. $ \\gamma $: $1.48397778$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344df0e",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aef894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Estimated parameters (two-step GMM): [0.10564767 0.12328659]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "file_path = '/Users/Eric/Downloads/Tbill_1mo.csv' \n",
    "interest_rate_data = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare the data by scaling the interest rates\n",
    "interest_rate_data['scaled_rate'] = interest_rate_data['ave_1'] / 100\n",
    "\n",
    "# Calculate the time step for monthly data\n",
    "delta_t = 1 / 12\n",
    "\n",
    "# Get the interest rates from the data\n",
    "interest_rates = interest_rate_data['scaled_rate'].values\n",
    "\n",
    "# Define the moment conditions with fixed alpha = 0 and gamma = 1\n",
    "def moment_conditions(params, rates, time_step):\n",
    "    beta, sigma_sq = params\n",
    "    residuals_first = rates[1:] - rates[:-1] - (beta * rates[:-1]) * time_step\n",
    "    residuals_second = residuals_first * rates[:-1]\n",
    "    residuals_third = (residuals_first**2 - sigma_sq * (rates[:-1]**2) * time_step)\n",
    "    all_residuals = np.column_stack((residuals_first, residuals_second, residuals_third))\n",
    "    return all_residuals\n",
    "\n",
    "# Define the objective function for the GMM first step\n",
    "def gmm_objective_step1(params, rates, time_step):\n",
    "    residuals = moment_conditions(params, rates, time_step)\n",
    "    weighting_matrix = np.eye(residuals.shape[1])\n",
    "    criterion_value = residuals.mean(axis=0) @ weighting_matrix @ residuals.mean(axis=0)\n",
    "    return criterion_value\n",
    "\n",
    "# Initial parameter guesses for beta and sigma^2\n",
    "initial_params = np.array([0.1, 0.1])\n",
    "\n",
    "# First step optimization with fixed alpha and gamma\n",
    "result_step1 = minimize(\n",
    "    fun=gmm_objective_step1,\n",
    "    x0=initial_params,\n",
    "    args=(interest_rates, delta_t),\n",
    "    method='L-BFGS-B',\n",
    "    bounds=[(None, None), (0, None)]\n",
    ")\n",
    "\n",
    "# Calculate the optimal weighting matrix using the residuals from the first step\n",
    "residuals_step1 = moment_conditions(result_step1.x, interest_rates, delta_t)\n",
    "optimal_weighting_matrix = np.linalg.inv(np.cov(residuals_step1.T))\n",
    "\n",
    "# Define the objective function for the GMM second step\n",
    "def gmm_objective_step2(params, rates, time_step, W):\n",
    "    residuals = moment_conditions(params, rates, time_step)\n",
    "    criterion_value = residuals.mean(axis=0) @ W @ residuals.mean(axis=0)\n",
    "    return criterion_value\n",
    "\n",
    "# Second step optimization using the optimal weighting matrix\n",
    "result_step2 = minimize(\n",
    "    fun=gmm_objective_step2,\n",
    "    x0=result_step1.x,\n",
    "    args=(interest_rates, delta_t, optimal_weighting_matrix),\n",
    "    method='L-BFGS-B',\n",
    "    bounds=[(None, None), (0, None)]\n",
    ")\n",
    "\n",
    "# Check the result and print the parameters\n",
    "if result_step2.success:\n",
    "    estimated_params = result_step2.x\n",
    "    print(f\"Estimated parameters (two-step GMM): {estimated_params}\")\n",
    "else:\n",
    "    raise Exception(\"Second step optimization failed: \" + result_step2.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02199244",
   "metadata": {},
   "source": [
    "The parameters for GBM model have been estimated using the two-step GMM approach. The estimated parameters are as follows:\n",
    "\n",
    "1. $\\beta$: $0.10564767$\n",
    "\n",
    "2. $\\sigma^2$: $0.12328659$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
