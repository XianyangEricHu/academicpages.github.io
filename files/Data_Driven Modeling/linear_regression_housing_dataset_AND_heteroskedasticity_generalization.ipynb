{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    \n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "        \n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    if not os.path.isfile(tgz_path): #download data if not already there\n",
    "        urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "        \n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on sklearn object design:\n",
    "\n",
    "    - Estimators: any object estimating some parameters. In the above, the imputer is an estimator. Estimators need to have a fit() method which take the dataset as input. Any other parameters are considered as hyperparameters, e.g. the strategy hyperparameter in the imputer\n",
    "\n",
    "    - Transformers: these are estimators which can transofrm the dataset. They need to implement the transform() method. All transformers also has a fit_transform() method equivalent to calling fit() and then transform(). Sometimes the fit_transform() method is better optimized for efficiency so usually best to call it instead of fit() and then transform(). The imputer above is actually a transformer.\n",
    "\n",
    "    - Predictors: these are estimators which can make predictions. LinearRegression model is a predictor. Predictors must implement a predict() method. They also have a score() method that measures the quality of the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a LinRegStatsmodels Predictor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class LinRegStatsmodels(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.model = None\n",
    "        self.results = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #adding a constant (intercept) to the model\n",
    "        if self.fit_intercept:\n",
    "            X = sm.add_constant(X)\n",
    "        #fitting the model\n",
    "        self.model = sm.OLS(y, X).fit()\n",
    "        #saving the results\n",
    "        self.results = self.model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        #adding a constant (intercept) to the model\n",
    "        if self.fit_intercept:\n",
    "            X = sm.add_constant(X)\n",
    "        return self.model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", full_pipeline),\n",
    "        (\"linear\", LinRegStatsmodels())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     median_house_value   R-squared:                       0.648\n",
      "Model:                            OLS   Adj. R-squared:                  0.648\n",
      "Method:                 Least Squares   F-statistic:                     2026.\n",
      "Date:                Tue, 03 Oct 2023   Prob (F-statistic):               0.00\n",
      "Time:                        23:50:18   Log-Likelihood:            -2.0731e+05\n",
      "No. Observations:               16512   AIC:                         4.147e+05\n",
      "Df Residuals:                   16496   BIC:                         4.148e+05\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.974e+05   8110.732     24.343      0.000    1.82e+05    2.13e+05\n",
      "x1         -5.565e+04   2292.802    -24.271      0.000   -6.01e+04   -5.12e+04\n",
      "x2         -5.671e+04   2415.248    -23.481      0.000   -6.14e+04    -5.2e+04\n",
      "x3          1.373e+04    616.492     22.279      0.000    1.25e+04    1.49e+04\n",
      "x4         -1943.0559   2190.645     -0.887      0.375   -6236.956    2350.844\n",
      "x5          7343.2298   3198.998      2.295      0.022    1072.849    1.36e+04\n",
      "x6         -4.571e+04   1374.626    -33.252      0.000   -4.84e+04    -4.3e+04\n",
      "x7          4.545e+04   3144.869     14.453      0.000    3.93e+04    5.16e+04\n",
      "x8          7.471e+04    755.292     98.921      0.000    7.32e+04    7.62e+04\n",
      "x9          6604.5840    675.767      9.773      0.000    5280.008    7929.159\n",
      "x10         1043.0545    553.122      1.886      0.059     -41.124    2127.233\n",
      "x11         9248.3161    791.406     11.686      0.000    7697.076    1.08e+04\n",
      "x12         2.147e+04   8138.589      2.638      0.008    5519.177    3.74e+04\n",
      "x13        -1.573e+04   8254.722     -1.905      0.057   -3.19e+04     453.112\n",
      "x14         1.498e+05   4.05e+04      3.702      0.000    7.05e+04    2.29e+05\n",
      "x15           1.7e+04   8254.816      2.060      0.039     822.688    3.32e+04\n",
      "x16         2.485e+04   8206.767      3.027      0.002    8759.042    4.09e+04\n",
      "==============================================================================\n",
      "Omnibus:                     4238.648   Durbin-Watson:                   1.982\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            20510.516\n",
      "Skew:                           1.164   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.939   Cond. No.                     2.81e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 8.17e-27. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "full_pipeline_with_predictor.fit(housing, housing_labels)\n",
    "\n",
    "print(full_pipeline_with_predictor.named_steps[\"linear\"].results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     median_house_value   R-squared:                       0.648\n",
      "Model:                            OLS   Adj. R-squared:                  0.648\n",
      "Method:                 Least Squares   F-statistic:                 1.044e+04\n",
      "Date:                Tue, 03 Oct 2023   Prob (F-statistic):               0.00\n",
      "Time:                        23:50:18   Log-Likelihood:            -2.0731e+05\n",
      "No. Observations:               16512   AIC:                         4.147e+05\n",
      "Df Residuals:                   16496   BIC:                         4.148e+05\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:                  HC3                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.974e+05   1.06e+04     18.626      0.000    1.77e+05    2.18e+05\n",
      "x1         -5.565e+04   2409.017    -23.101      0.000   -6.04e+04   -5.09e+04\n",
      "x2         -5.671e+04   2646.962    -21.425      0.000   -6.19e+04   -5.15e+04\n",
      "x3          1.373e+04    765.036     17.953      0.000    1.22e+04    1.52e+04\n",
      "x4         -1943.0559   6025.008     -0.322      0.747   -1.38e+04    9866.610\n",
      "x5          7343.2298   7793.967      0.942      0.346   -7933.785    2.26e+04\n",
      "x6         -4.571e+04   8156.456     -5.604      0.000   -6.17e+04   -2.97e+04\n",
      "x7          4.545e+04   8301.661      5.475      0.000    2.92e+04    6.17e+04\n",
      "x8          7.471e+04   1916.863     38.977      0.000     7.1e+04    7.85e+04\n",
      "x9          6604.5840   2138.730      3.088      0.002    2412.442    1.08e+04\n",
      "x10         1043.0545   2626.546      0.397      0.691   -4105.259    6191.368\n",
      "x11         9248.3161   4296.328      2.153      0.031     827.049    1.77e+04\n",
      "x12         2.147e+04   1.06e+04      2.021      0.043     644.956    4.23e+04\n",
      "x13        -1.573e+04   1.08e+04     -1.457      0.145   -3.69e+04    5428.390\n",
      "x14         1.498e+05   5.29e+04      2.833      0.005    4.62e+04    2.54e+05\n",
      "x15           1.7e+04   1.07e+04      1.585      0.113   -4022.769     3.8e+04\n",
      "x16         2.485e+04   1.07e+04      2.326      0.020    3904.471    4.58e+04\n",
      "==============================================================================\n",
      "Omnibus:                     4238.648   Durbin-Watson:                   1.982\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            20510.516\n",
      "Skew:                           1.164   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.939   Cond. No.                     2.81e+15\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
      "[2] The smallest eigenvalue is 8.17e-27. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "heteroskedastic_results = full_pipeline_with_predictor.named_steps[\"linear\"].results.get_robustcov_results(cov_type='HC3')\n",
    "\n",
    "print(heteroskedastic_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "The model with homoscedasticity appears to have a relatively good fit, explaining about 64.8% of the variance in median_house_value. Several predictors are statistically significant, given their p-values, but it's crucial to understand what each predictor represents for practical interpretation. However, based on the Omnibus and Jarque-Bera tests, the residuals are not normally distributed, which might be a concern. Additionally, the high condition number suggests potential multicollinearity among the predictors, which might mean some predictors are redundant or correlated.\n",
    "\n",
    "For the model with heteroskedastic standard errors, the coefficients' estimates remain the same, but the standard errors have changed, affecting their significance levels. This indicates the importance of accounting for heteroscedasticity when it's present, as it can affect the conclusions about the importance of some predictors. A few variables (like x10 and x5) which seemed significant or borderline significant in the initial model, now appear to be much less significant in the heteroscedasticity-corrected model. This emphasizes that failing to account for heteroscedasticity can lead to incorrect inferences about the significance of predictors.\n",
    "\n",
    "Overall, the heteroscedasticity-corrected model offers a more reliable interpretation of the predictors' significance and their standard errors. The model fit measures like R-squared remain unchanged, but the conclusions regarding individual predictors' importance have been adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_income_value = housing[\"median_income\"].median()\n",
    "new_data = pd.DataFrame({\n",
    "    'longitude': [-118.8],\n",
    "    'latitude': [34.19],\n",
    "    'housing_median_age': [4.0],\n",
    "    'total_rooms': [15572.0],\n",
    "    'total_bedrooms': [2222.0],\n",
    "    'population': [5495.0],\n",
    "    'households': [2152.0],\n",
    "    'median_income': [median_income_value],\n",
    "    'ocean_proximity': [\"<1H OCEAN\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted House Value: [241862.50684923]\n"
     ]
    }
   ],
   "source": [
    "prepared_new_data = full_pipeline.transform(new_data)\n",
    "\n",
    "predicted_value = lin_reg.predict(prepared_new_data)\n",
    "print(f\"Predicted House Value: {predicted_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to estimate\n",
    "\n",
    "The feature median_income and housing_median_age are missing in the data given. To make the data fit for the pipeline, I used the median of median_income and housing age to substitute these two feature. The reason that I used the median of all median_incomes for median_income is to mimimize the impact of this factor in prediction without changing the trained model. The reason that I used housing age to replace the housing_median_age is because I considered them to be of similar meaning in the context. These two features might affect the prediction accuracy in the final result, but confidence intervals can help with our prediction. \n",
    "\n",
    "Since I used sklearn's LinearRegression model trained in the lecture codes, which doesn't provide a built-in way to get prediction intervals or confidence intervals, I will have to compute confidence intervals by hand using the model's coefficients, residuals, and the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizations: Heteroskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff5 = pd.read_csv('/Users/Eric/opt/anaconda3/envs/dsm/F-F_Research_Data_5_Factors_2x3_daily.CSV', skiprows=3)\n",
    "qmnix = pd.read_csv('/Users/Eric/opt/anaconda3/envs/dsm/QMNIX.csv')\n",
    "qmnix['Return'] = qmnix['Adj Close'].pct_change()\n",
    "ff5['Date'] = pd.to_datetime(ff5['Unnamed: 0'].astype(str), format='%Y%m%d')\n",
    "qmnix['Date'] = pd.to_datetime(qmnix['Date'])\n",
    "qmnix = qmnix[['Date','Return']]\n",
    "ff5.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "merged_data = pd.merge(ff5, qmnix, on='Date', how='inner')\n",
    "merged_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Return   R-squared:                       0.239\n",
      "Model:                            OLS   Adj. R-squared:                  0.237\n",
      "Method:                 Least Squares   F-statistic:                     75.58\n",
      "Date:                Tue, 03 Oct 2023   Prob (F-statistic):           2.77e-73\n",
      "Time:                        23:50:18   Log-Likelihood:                 8736.4\n",
      "No. Observations:                2215   AIC:                        -1.746e+04\n",
      "Df Residuals:                    2209   BIC:                        -1.743e+04\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:                  HC3                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0001      0.000      1.293      0.196   -6.69e-05       0.000\n",
      "Mkt-RF         0.0002      0.000      1.581      0.114   -5.63e-05       0.001\n",
      "SMB           -0.0010      0.000     -4.767      0.000      -0.001      -0.001\n",
      "HML            0.0012      0.000      4.824      0.000       0.001       0.002\n",
      "RMW            0.0025      0.000      9.925      0.000       0.002       0.003\n",
      "CMA            0.0019      0.000      5.066      0.000       0.001       0.003\n",
      "==============================================================================\n",
      "Omnibus:                       99.118   Durbin-Watson:                   1.844\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              259.300\n",
      "Skew:                          -0.210   Prob(JB):                     4.94e-57\n",
      "Kurtosis:                       4.623   Cond. No.                         3.84\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n"
     ]
    }
   ],
   "source": [
    "y = merged_data['Return']\n",
    "\n",
    "X = merged_data[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "\n",
    "regressor = LinRegStatsmodels()\n",
    "\n",
    "# Fit the model with heteroskedasticity-robust standard errors\n",
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_const)\n",
    "results = model.fit(cov_type='HC3')  # heteroskedasticity-robust standard errors\n",
    "\n",
    "regressor.model = model\n",
    "regressor.results = results\n",
    "\n",
    "print(regressor.results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences:\n",
    "\n",
    "**Covariance Type**: \n",
    "\n",
    "This regression used heteroskedasticity-robust standard errors (`Covariance Type: HC3`). This method provides standard errors that are robust to violations of the homoscedasticity assumption. \n",
    "The previous regression used non-robust standard errors (`Covariance Type: nonrobust`). This means that the standard errors in the previous regression may be biased if there's heteroskedasticity in the residuals.\n",
    "\n",
    "**Coefficients and Significance**:\n",
    "\n",
    "The coefficients (coef) of the factors in both regressions are fairly similar.\n",
    "However, when comparing the significance levels (P>|z| vs P>|t|), some discrepancies arise. For instance, the factor `Mkt-RF` is significant at the 10% level in the previous regression but not in the this one. \n",
    "\n",
    "**Model Metrics**:\n",
    "  \n",
    "The `R-squared` values are the same in both regressions, meaning that the proportion of variance explained by the regressors remains consistent across both models.\n",
    "The `Adj. R-squared` also remains consistent, which adjusts the R-squared value based on the number of predictors in the model.\n",
    "The F-statistic is different between the two models, but both have extremely low Prob (F-statistic) values, implying the joint significance of the coefficients.\n",
    "\n",
    "**Test Statistics**:\n",
    "\n",
    "Both regressions have Omnibus, Durbin-Watson, Jarque-Bera, Skew, and Kurtosis statistics. The values of these statistics are almost the same, meaning that the overall characteristics of the residuals (like their distribution, autocorrelation, etc.) remain similar.\n",
    "\n",
    "The main differences are in the standard errors due to heteroskedasticity adjustment. The model above is better equipped to handle potential heteroskedasticity in the data. The model with homoscedasticity assumption offers a more parsimonious approach and could be preferred if heteroskedasticity is not a concern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
