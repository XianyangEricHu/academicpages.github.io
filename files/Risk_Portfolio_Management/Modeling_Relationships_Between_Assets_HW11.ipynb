{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597f004f",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "For this problem, use the same British Pound log-return data as you did in Problem 5 from HW10. Modify the Garch11Fit function so that it fits parameters $c_1 b_1, a_1$ and $a_1^{-}$of a $T G A R C H(9.45)$ model to the training set. This is the main model used by the NYU Volatility Lab https://vlab.stern.nyu.edu/. As in Problem 5 from HW10, form standard deviations $\\sigma_i$ over your validation set using the TGARCH parameters from the training set, and form the time series $y_i /$ sigma $_i$ in the validation period. How much of a difference does TGARCH make (compared to GARCH) in terms of the $y_i / \\sigma_i$ variance and kurtosis targets in the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6992067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "TGARCH Normalized Variance: 1.0505730063656\n",
      "TGARCH Excess Kurtosis: 0.9279379107744421\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/Eric/opt/anaconda3/envs/dsm/3C.csv')\n",
    "\n",
    "# Assuming df is already loaded with DEXUSUK and DATE columns\n",
    "df['DEXUSUK'] = pd.to_numeric(df['DEXUSUK'], errors='coerce').dropna()\n",
    "df = df[df['DEXUSUK'] > 0]\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "df['log_returns'] = np.log(df['DEXUSUK']).diff()\n",
    "df = df.dropna()\n",
    "\n",
    "# Splitting the data\n",
    "training_set = df.loc['2017-01-01':'2020-12-31', 'log_returns']\n",
    "validation_set = df.loc['2021-01-01':'2021-12-31', 'log_returns']\n",
    "\n",
    "# Fit the TGARCH(9,45) model\n",
    "tgarch_model = arch_model(training_set, p=1, q=1, o=1, power=9.45)\n",
    "tgarch_result = tgarch_model.fit(update_freq=5, disp='off')\n",
    "\n",
    "# Forecasting Volatility for the entire period\n",
    "forecast = tgarch_result.forecast(start=training_set.index[0], horizon=1, reindex=False)\n",
    "sigmas_full = np.sqrt(forecast.variance.dropna().values[:, 0])\n",
    "\n",
    "# Select the forecasted volatilities for the validation period\n",
    "sigmas_validation = sigmas_full[-len(validation_set):]\n",
    "\n",
    "# Ensure the lengths match\n",
    "if len(sigmas_validation) != len(validation_set):\n",
    "    raise ValueError(\"Mismatch in length of forecasted volatilities and validation set.\")\n",
    "\n",
    "# Normalized Returns\n",
    "normalized_returns_tgarch = validation_set / sigmas_validation\n",
    "\n",
    "# Variance and Kurtosis\n",
    "variance_tgarch = np.var(normalized_returns_tgarch)\n",
    "kurtosis_tgarch = normalized_returns_tgarch.kurtosis()\n",
    "\n",
    "# Display Results\n",
    "print(f'TGARCH Normalized Variance: {variance_tgarch}')\n",
    "print(f'TGARCH Excess Kurtosis: {kurtosis_tgarch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f85342",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "1. **GARCH Model Results:**\n",
    "   - Normalized Variance: $0.9011$\n",
    "   - Excess Kurtosis: $0.6636$\n",
    "\n",
    "2. **TGARCH Model Results:**\n",
    "   - Normalized Variance: $1.0506$\n",
    "   - Excess Kurtosis: $0.9279$\n",
    "\n",
    "#### Normalized Variance Comparison:\n",
    "- The GARCH model has a normalized variance of $0.9011$. This is below 1, indicating that the model is overestimating market volatility.\n",
    "- The TGARCH model's normalized variance is $1.0506$, slightly above 1, suggesting a minor underestimation of market volatility. This model is closer to the ideal value, indicating a better fit for volatility estimation.\n",
    "\n",
    "#### Excess Kurtosis Comparison:\n",
    "- The GARCH model shows an excess kurtosis of $0.6636$. This positive value indicates a leptokurtic distribution, meaning the distribution of normalized returns has fatter tails than a normal distribution.\n",
    "- The TGARCH model, with an excess kurtosis of $0.9279$, also indicates a leptokurtic distribution but suggests even fatter tails than the GARCH model. This implies a higher likelihood of extreme market movements.\n",
    "\n",
    "#### Interpretation:\n",
    "- The TGARCH model's ability to account for asymmetry in volatility (different impacts of positive and negative shocks) results in a slightly more accurate estimation of market volatility compared to the GARCH model, as seen in the normalized variance values.\n",
    "- Both models show a leptokurtic distribution for normalized returns, but the TGARCH model indicates a higher likelihood of extreme price movements (fatter tails) than the GARCH model. This could be due to the TGARCH model's enhanced sensitivity to market downturns, often characterized by higher volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9c768",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Let $D, L$, and $V$ be the currency data, learning set, and validation set as in Problem 1 from Class $9 \\mathrm{HW}$. Estimate the correlation matrix in $L$ using (a) (equal-weighted) historical data $\\left(R_{L, h}\\right)$; and (b) EWMA data with a half-life of one year $\\left(R_{L, 1}\\right)$. Compute the sample correlation matrix in $V\\left(R_V\\right)$. (a) Using the Box M Test from Chapter 4 (see Class $9 \\mathrm{HW}$, Problem 4), which of $R_{L, h}$ and $R_{L, 1}$ has a higher $p$-value with $R_H$ ? (b) Find to within 20 days the optimal $h<500$ days so that $R_{L, h}$ (EWMA correlation matrix from $L$ with halflife $h$ ) has the highest $p$-value with $R_H$. (Yes, that's overfitting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb38c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix R_L_1 has a higher p-value of 0.07509653246604775 when compared with R_V.\n",
      "Optimal Half-life: 20 days\n",
      "Highest p-value: 0.004134235665970043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Eric/opt/anaconda3/envs/dsm/lib/python3.11/site-packages/pandas/core/internals/blocks.py:329: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/Eric/opt/anaconda3/envs/dsm/lib/python3.11/site-packages/pandas/core/internals/blocks.py:329: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/Eric/opt/anaconda3/envs/dsm/lib/python3.11/site-packages/pandas/core/internals/blocks.py:329: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/Users/Eric/opt/anaconda3/envs/dsm/lib/python3.11/site-packages/pandas/core/internals/blocks.py:329: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as spst\n",
    "\n",
    "# Load the data\n",
    "file_path = '/Users/Eric/opt/anaconda3/envs/dsm/3C.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Split the data into learning and validation sets\n",
    "last_full_year = data['Date'].dt.year.max()\n",
    "learning_set = data[data['Date'].dt.year < last_full_year]\n",
    "validation_set = data[data['Date'].dt.year == last_full_year]\n",
    "\n",
    "# Function to compute log returns\n",
    "def compute_log_returns(data):\n",
    "    return np.log(data / data.shift(1)).dropna()\n",
    "\n",
    "# Compute log returns for learning and validation sets\n",
    "log_returns_learning = compute_log_returns(learning_set.iloc[:, 1:])\n",
    "log_returns_validation = compute_log_returns(validation_set.iloc[:, 1:])\n",
    "\n",
    "# Calculate correlation matrices\n",
    "R_L_h = log_returns_learning.corr()\n",
    "R_V = log_returns_validation.corr()\n",
    "\n",
    "# EWMA correlation matrix function\n",
    "def ewma_correlation(log_returns, halflife):\n",
    "    decay_factor = 1 - np.exp(np.log(0.5) / halflife)\n",
    "    ewma_cov = log_returns.ewm(alpha=decay_factor).cov()\n",
    "    last_date = log_returns.index[-1]\n",
    "    ewma_cov_last = ewma_cov.loc[last_date]\n",
    "    std_dev = np.sqrt(np.diag(ewma_cov_last))\n",
    "    corr_matrix = ewma_cov_last.div(std_dev, axis=0).div(std_dev, axis=1)\n",
    "    return corr_matrix\n",
    "\n",
    "# Calculate the EWMA correlation matrix for the learning set with a half-life of one year (252 trading days)\n",
    "R_L_1 = ewma_correlation(log_returns_learning, halflife=252)\n",
    "\n",
    "# Box M Test function\n",
    "def BoxM(T1, T2, s1, s2):\n",
    "    # Ensure dimensions are the same\n",
    "    if len(s1) != len(s2):\n",
    "        print(\"Error: different dimensions in Box M Test:\", len(s1), len(s2))\n",
    "        return (0, 0)\n",
    "    \n",
    "    # Matrices are pxp\n",
    "    p = len(s1)\n",
    "\n",
    "    # Form the combined matrix\n",
    "    scomb = (T1 * s1 + T2 * s2) / (T1 + T2)\n",
    "\n",
    "    # Box M statistic\n",
    "    Mstat = (T1 + T2 - 2) * np.log(np.linalg.det(scomb)) - \\\n",
    "            (T1 - 1) * np.log(np.linalg.det(s1)) - (T2 - 1) * np.log(np.linalg.det(s2))\n",
    "\n",
    "    # Calculating multipliers\n",
    "    A1 = (2 * p ** 2 + 3 * p - 1) / (6 * (p + 1)) * (1 / (T1 - 1) + 1 / (T2 - 1) - 1 / (T1 + T2 - 2))\n",
    "    A2 = (p - 1) * (p + 2) / 6 * (1 / (T1 - 1) ** 2 + 1 / (T2 - 1) ** 2 - 1 / (T1 + T2 - 2) ** 2)\n",
    "\n",
    "    # Calculate degrees of freedom and discriminant\n",
    "    discrim = A2 - A1 ** 2\n",
    "    df1 = p * (p + 1) / 2\n",
    "\n",
    "    if discrim <= 0:\n",
    "        # Use chi-square distribution\n",
    "        test_value = Mstat * (1 - A1)\n",
    "        p_value = 1 - spst.chi2.cdf(test_value, df1)\n",
    "    else:\n",
    "        # Use F distribution\n",
    "        df2 = (df1 + 2) / discrim\n",
    "        b = df1 / (1 - A1 - (df1 / df2))\n",
    "        test_value = Mstat / b\n",
    "        p_value = 1 - spst.f.cdf(test_value, df1, df2)\n",
    "\n",
    "    return (test_value, p_value)\n",
    "\n",
    "# Perform Box M Test for both R_L_h and R_L_1 against R_V\n",
    "test_value_h, p_value_h = BoxM(len(learning_set), len(validation_set), R_L_h, R_V)\n",
    "test_value_1, p_value_1 = BoxM(len(learning_set), len(validation_set), R_L_1, R_V)\n",
    "\n",
    "# Determine which has a higher p-value\n",
    "higher_p_value_matrix = \"R_L_h\" if p_value_h > p_value_1 else \"R_L_1\"\n",
    "higher_p_value = max(p_value_h, p_value_1)\n",
    "\n",
    "print(f\"The matrix {higher_p_value_matrix} has a higher p-value of {higher_p_value} when compared with R_V.\")\n",
    "\n",
    "# Function to find optimal half-life\n",
    "def find_optimal_h(log_returns_learning, log_returns_validation, max_days=500):\n",
    "    optimal_h = 0\n",
    "    max_p_value = 0\n",
    "    for h in range(1, max_days + 1):\n",
    "        R_L_h_current = ewma_correlation(log_returns_learning, halflife=h)\n",
    "        _, p_value_current = BoxM(len(learning_set), len(validation_set), R_L_h_current, R_V)\n",
    "        if p_value_current > max_p_value:\n",
    "            max_p_value = p_value_current\n",
    "            optimal_h = h\n",
    "    return optimal_h, max_p_value\n",
    "\n",
    "# Find the optimal h\n",
    "optimal_h, optimal_p_value = find_optimal_h(log_returns_learning, log_returns_validation, max_days=20)\n",
    "\n",
    "print(f\"Optimal Half-life: {optimal_h} days\")\n",
    "print(f\"Highest p-value: {optimal_p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0ac7a",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "The joint distribution of default times $\\tau_1, \\tau_2$ of two bonds is given by\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}\\left(\\tau_1 \\leq T_1, \\tau_2 \\leq T_2\\right)=C\\left(L\\left(T_1\\right), L\\left(T_2\\right)\\right)\n",
    "$$\n",
    "\n",
    "for times $0 \\leq T_1, T_2$ where $C(u, v)=u v(1+3 \\rho(1-u)(1-v))$ and $L()$ is the cdf of a standard logLaplace distribution (standard Laplace with pdf $\\frac{1}{\\sqrt{2}} \\exp (-\\sqrt{2}|x|)$ on the logarithm of time). What is the Spearman rank correlation between the bonds' default times? (Hint: read the paragraph after equation (10.13).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc668f",
   "metadata": {},
   "source": [
    "To find the Spearman rank correlation between the bonds' default times, we can use the relationship between Spearman's rank correlation and the copula used to model the joint distribution of the default times. The Spearman rank correlation, denoted as $\\rho_S$, can be computed from the copula $C(u, v)$ using the following formula:\n",
    "\n",
    "$$\n",
    "\\rho_S = 12 \\int_0^1 \\int_0^1 C(u, v) \\, du \\, dv - 3\n",
    "$$\n",
    "\n",
    "Given the copula function $C(u, v) = uv(1 + 3\\rho(1 - u)(1 - v))$, we need to integrate this over the unit square $[0,1] \\times [0,1]$. \n",
    "\n",
    "The Spearman rank correlation between the bonds' default times, given by the copula function $C(u, v) = uv(1 + 3\\rho(1 - u)(1 - v))$, is exactly equal to $\\rho$, the parameter in the copula function. This result indicates a direct relationship between the Spearman rank correlation and the parameter $\\rho$ in the specified copula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16037c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\rho$"
      ],
      "text/plain": [
       "rho"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, integrate\n",
    "\n",
    "# Defining the variables\n",
    "u, v, rho = symbols('u v rho')\n",
    "C = u * v * (1 + 3 * rho * (1 - u) * (1 - v))\n",
    "\n",
    "# Computing the double integral of the copula over the unit square [0, 1] x [0, 1]\n",
    "integral_result = integrate(integrate(C, (u, 0, 1)), (v, 0, 1))\n",
    "\n",
    "# Calculating the Spearman rank correlation\n",
    "spearman_rank_correlation = 12 * integral_result - 3\n",
    "spearman_rank_correlation.simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e96f0f1",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Consider the Gaussian copula $G(u, v)=\\operatorname{Norm}\\left(N^{-1}(u), N^{-1}(v)\\right)$. Here $\\operatorname{Norm}(x, y)$ is a bivariate normal distribution with mean 0 and covariance matrix $R$, where $R$ has 1 on the diagonal and $\\rho$ off the diagonal. $N(x)$ is the standard normal cdf and $N^{-1}(N(x))=x$. Verify (by setting up and computing the appropriate multiple integral) that $G$ is in fact a copula function, i.e. that its marginals are uniform; it is nondecreasing; and it has the appropriate domain and range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c9cf5",
   "metadata": {},
   "source": [
    "Given the Gaussian copula:\n",
    "\n",
    "$$ G(u, v) = \\operatorname{Norm}\\left(N^{-1}(u), N^{-1}(v)\\right) $$\n",
    "\n",
    "where $\\operatorname{Norm}$ is the bivariate normal distribution function with mean 0 and covariance matrix $R$ (with 1s on the diagonal and $\\rho$ off the diagonal), and $N$ is the standard normal CDF.\n",
    "\n",
    "### 1. Uniform Marginals\n",
    "\n",
    "For $G(u, v)$ to have uniform marginals, the integration over one variable must yield the other variable. For the uniform distribution, this means:\n",
    "$$ \\int_0^1 G(u, v) \\, dv = u $$\n",
    "$$ \\int_0^1 G(u, v) \\, du = v $$\n",
    "\n",
    "We need to set up and compute these integrals.\n",
    "\n",
    "For $ u $ marginal:\n",
    "$$ \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} e^{-\\frac{x^2 - 2\\rho x N^{-1}(v) + (N^{-1}(v))^2}{2(1-\\rho^2)}} dx = v $$\n",
    "\n",
    "For $v$ marginal:\n",
    "$$ \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} e^{-\\frac{(N^{-1}(u))^2 - 2\\rho N^{-1}(u) y + y^2}{2(1-\\rho^2)}} dy = u $$\n",
    "\n",
    "### 2. Non-decreasing\n",
    "\n",
    "To verify the non-decreasing property, we need to show that:\n",
    "$$ \\frac{\\partial G}{\\partial u} \\geq 0 $$\n",
    "\n",
    "$$ \\frac{\\partial G}{\\partial v} \\geq 0 $$\n",
    "\n",
    "This needs to be true for all $(u, v) \\in [0, 1] \\times [0, 1]$.\n",
    "\n",
    "This property is satisfied if the copula function is non-decreasing in both arguments. For a Gaussian copula, this property holds because the normal CDF $ N(x) $ is a nondecreasing function, and the covariance matrix $ R $ defines a nondecreasing bivariate normal distribution. Since both are satisfied, it is non-decreasing.\n",
    "\n",
    "### 3. Appropriate Domain and Range\n",
    "\n",
    "The domain of a copula function is the unit square $[0, 1] \\times [0, 1]$, and the range is the unit interval $[0, 1]$. For $ G(u, v) $, this is true because the inputs $ u $ and $ v $ are transformed by the inverse of the normal CDF, which maps $[0, 1]$ to the entire real line $ (-\\infty, +\\infty) $, and then through the bivariate normal distribution, which maps back to $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a26daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal distribution of U at 0.5: 0.12030982838508406\n",
      "Marginal distribution of V at 0.5: 0.12030982838508406\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "\n",
    "# Define the covariance matrix R\n",
    "rho = 0.5  # Correlation coefficient\n",
    "R = np.array([[1, rho], [rho, 1]])\n",
    "\n",
    "# Gaussian copula density function\n",
    "def gaussian_copula_density(u, v, rho):\n",
    "    x = norm.ppf(u)\n",
    "    y = norm.ppf(v)\n",
    "    z = np.array([x, y])\n",
    "    return multivariate_normal.pdf(z, mean=[0, 0], cov=R)\n",
    "\n",
    "# Marginal distribution function for u\n",
    "def marginal_u(u):\n",
    "    return quad(lambda v: gaussian_copula_density(u, v, rho), 0, 1)[0]\n",
    "\n",
    "# Marginal distribution function for v\n",
    "def marginal_v(v):\n",
    "    return quad(lambda u: gaussian_copula_density(u, v, rho), 0, 1)[0]\n",
    "\n",
    "# Check if marginals are uniform at a point (e.g., 0.5)\n",
    "u_check = v_check = 0.5\n",
    "marginal_u_result = marginal_u(u_check)\n",
    "marginal_v_result = marginal_v(v_check)\n",
    "\n",
    "print(f\"Marginal distribution of U at {u_check}: {marginal_u_result}\")\n",
    "print(f\"Marginal distribution of V at {v_check}: {marginal_v_result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
